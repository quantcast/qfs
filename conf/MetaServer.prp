#
# $Id#
#
# Copyright 2008-2012 Quantcast Corp.
#
# Author: Mike Ovsiannikov
#
# This file is part of Kosmos File System (KFS).
#
# Licensed under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License. You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied. See the License for the specific language governing
# permissions and limitations under the License.

# Client listener port.
metaServer.clientPort = 20000

# Chunk server listener port.
metaServer.chunkServerPort = 30000

# Meta serve transactions log directory.
metaServer.logDir = meta/transaction_logs

# Meta server checkpoint directory.
metaServer.cpDir = meta/checkpoint

# Allow to automatically create an empty file system if checkpoint file does
# not exist.
# The default is 0, as under the normal circumstances where the file system
# content is of value, completely losing checkpoint, transaction log, and
# automatically creating an empty fs will have the same effect as conventional
# "mkfs". All chunks (blocks) will get deleted, and restoring the checkpoint
# and logs later won't be sufficient to recover the data.
metaServer.createEmptyFs = 1

# Root directory permissions -- used only when the new file system created.
# metaServer.rootDirUser  = 0
# metaServer.rootDirGroup = 0
# metaServer.rootDirMode  = 0755

# Defaults for checkpoint and transaction log without permissions conversion on
# startup.
# metaServer.defaultLoadUser     = 0
# metaServer.defaultLoadGroup    = 0
# metaServer.defaultLoadFileMode = 0644
# metaServer.defaultLoadDirMode  = 0755

# The size of the "client" thread pool.
# When set to greater than 0, dedicated threads to do client network io, request
# parsing, and response assembly are created. The thread pool size should
# usually be (at least one) less than the number of cpus. "Client" threads help
# with processing large amount of ["short"] requests where more cpu used for
# context switch, network io, request parsing, and response assembly, than
# the cpu for the request processing itself. For example i-node attribute
# lookup, or write append chunk allocations that can be satisfied from the write
# append allocation cache.
# Default is 0 -- no dedicated "client" threads.
# metaServer.clientThreadCount = 0

# Meta server threads affinity.
# Presently only supported on linux.
# The first cpu index to set thread affinity to.
# The main thread will be assigned to the cpu at the specified index, then the
# next "client" thread will be assigned to the cpu index plus one and so on.
# For example with 2 client threads and start cpu index 0 the threads affinity
# would be 0 1 2 respectively.
# Useful on machines with more than one multi-core processor with shared dram
# cache. Assigning the threads to the same processor might help minimize dram
# cache misses.
# Default is off (start index less than 0) no thread affinity set.
# metaServer.clientThreadStartCpuAffinity = -1

# Meta server process max. locked memory.
# If set to a value greater than 0 then locked memory limit will be set to the
# specified value, and mlock(MCL_CURRENT|MCL_FUTURE) invoked.
# On linux running under non root user setting locked memory "hard" limit
# greater or equal to the specified value required. ulimit -l can be used for
# example.
# Default is 0 -- no memory locking.
# metaServer.maxLockedMemory = 0

# Size of [network] io buffer pool.
# The default buffer size is 4K, therefore the amount of memory is
# 4K * metaServer.bufferPool.partionBuffers.
# All io buffers are allocated at startup.
# If memory locking enabled io buffers are locked in memory at startup.
# Default is 256K or 1GB on 64 bit system, and 32K or 128MB on 32 bit system.
# metaServer.bufferPool.partionBuffers = 262144

# ==============================================================================
# The parameters below this line can be changed at runtime by editing the
# configuration file and sending meta server process HUP signal.
# Note that to restore parameter to default at run time the default value must
# be explicitly specified in the configuration file. In other words commenting
# out the parameter will not have any effect until restart.

# WORM mode.
# Write once, read many mode.
# In this mode only modification of files ".tmp" (without quotes) suffix is
# allowed.
# Typically the application would create and write the file with ".tmp" suffix,
# and then rename it so the destination file name will not have ".tmp " suffix.
# To delete a file without ".tmp" suffix the mode can be temporary turned off
# by the administrator. "qfstoggleworm" utility, or temporary configuration
# modification can be used to do that.
# Default is 0.
# metaServer.wormMode = 0

# Mininum number of connected / functional chunk servers before the file system
# can be used.
# Default is 1.
# metaServer.minChunkservers = 1

# Wait 30 sec for chunk servers to connect back after restarting, before file
# system considered fully functional.
metaServer.recoveryInterval = 30

# For write append use the low order bit of the IP address for the chunk servers
# master/slave assignment. This scheme is works well if least significant bit of
# ip address uniformly distributes masters and slaves withing the rack,
# especially with "in rack" placement for append.
# Default is 0. Assign master / slave to keep number of masters and slaves
# equal. The obvious downside of this is that the assignment depends on the
# chunk servers connection order.
# metaServer.assignMasterByIp = 0

# Chunk server executables md5 sums white list.
# The chunk server sends its executable md5sum when it connects to the meta
# server. If the following space separated list is not empty and does not
# does not contain the chunk server executable md5 sum then the chunk server
# is instructed to exit or restart itself.
# This might be useful for upgrade or versions control.
# While the chunk server is connected to the meta server no md5 sum verification
# performed.
# Default is empty list.
# metaServer.chunkServerMd5sums =

# Unique file system id -- some name that uniquely identifies distributed file
# system instance.
# This is used to protect data loss / and or corruption in the case where chunk
# server(s) connect to the "wrong" meta server.
# The meta server will not accept connections from the chunk servers with a
# different "cluster key".
# Default is empty string.
metaServer.clusterKey = my-fs-unique-identifier

# Assign rack id by ip prefix -- ip address treated as string.
# The prefix can be positioned with trailing ??
# For example: 10.6.34.2?
# The rack id assigned on chunk server connect, and will not change until the
# chunk server re-connect. Therefore the configuration file changes will not
# have any effect until the chunk servers re-connect.
# Default is empty -- use rack id assigned in the chunk server config.
# metaServer.rackPrefixes =
# Example:
# 10.6.1.* -- rack 1, 10.6.2.* -- rack 2, 10.6.4.1? -- rack 4 etc.
# metaServer.rackPrefixes = 10.6.1. 1  10.6.2. 2  10.6.4.1? 4  10.6.4.1 5

# "Static" placement weights of the racks. The more weight and more chunk
# servers are in the rack the more likely the rack will be chosen for chunk
# allocation.
# Default is empty -- all weight are default to 1.
# metaServer.rackWeights =
# Example: Racks 1 and 2 have weight 1, rack 3 -- 0.9, rack 4 weight 1.2,
# rack 5 weight 1.5. All other rack weights are 1.
# metaServer.rackWeights = 1 1  2 1  3 0.9  4 1.2  5 1.5

# Various timeout settings.

# Extend write lease expiration time by 30 sec. in the case of the write master
# disconnect, to give it a chance to re-connect.
# Default is 30 sec. Production value is 60 sec.
# metaServer.leaseOwnerDownExpireDelay = 30

# Re-replication or recovery delay in seconds on chunk server down, to give
# chunk server a chance to re-connect.
# Default is 120 sec.
# metaServer.serverDownReplicationDelay = 120

# Chunk server heartbeat interval.
# Default is 30 sec.
# metaServer.chunkServer.heartbeatInterval = 30

# Chunk server operations timeouts.
# Heartbeat timeout results in declaring chunk server non operational, and
# closing connection.
# All other operations timeout are interpreted as the operation failure.
# The values are in seconds.
# The defaults:
# metaServer.chunkServer.heartbeatTimeout    = 60
# metaServer.chunkServer.chunkReallocTimeout = 75
# metaServer.chunkServer.chunkAllocTimeout   = 40
# metaServer.chunkServer.chunkReallocTimeout = 75
# metaServer.chunkServer.makeStableTimeout   = 330
# metaServer.chunkServer.replicationTimeout  = 330

# The current production values.
# metaServer.chunkServer.heartbeatInterval   = 18
# metaServer.chunkServer.heartbeatTimeout    = 30
# metaServer.chunkServer.chunkReallocTimeout = 18
# metaServer.chunkServer.chunkAllocTimeout   = 18
# metaServer.chunkServer.makeStableTimeout   = 60

# Other chunk server operations timeout.
# metaServer.chunkServer.requestTimeout      = 600

# Chunk server space utilization placement threshold.
# Chunk servers with space utilization over this threshold are not considered
# as candidates for the chunk placement.
# Default is 0.95 or 95%.
# metaServer.maxSpaceUtilizationThreshold = 0.95

# Unix style permissions
# Space separated list of ip addresses of hosts where root user is allowed.
# Empty list means that root user is allowed on any host.
# Default is empty.
# metaServer.rootHosts =

# File modification time update resolution. Increasing the value will reduce the
# transaction log writes with large files.
# Default is 1 sec.
# metaServer.MTimeUpdateResolution = 1

# Force effective user to root. This effectively turns off all permissions
# control.
# Default is off.
# metaServer.forceEUserToRoot = 0

# Client backward compatibility.
# Defaults are no user and no group -- no backward compatibility.
# metaServer.defaultUser     = 0xFFFFFFFF
# metaServer.defaultGroup    = 0xFFFFFFFF
# metaServer.defaultFileMode = 0644
# metaServer.defaultDirMode  = 0755

# The chunk server disconnects history size. Useful for monitoring.
# Default is 4096 slots / disconnect events.
# metaServer.maxDownServersHistorySize = 4096

# Space and placement re-balancing.
# Space re-balancing is controlled by the next two parameters (thresholds) below.
# Re-balancing constantly scans all chunks in the system and checks chunk
# placement within the replication or RS groups, and moves chunks from chunk
# servers that are above metaServer.maxRebalanceSpaceUtilThreshold to the chunk
# servers that are below metaServer.minRebalanceSpaceUtilThreshold.
# Default is 1 -- on.
# metaServer.rebalancingEnabled = 1

# Space re-balancing thresholds.
# Move chunk from the servers that exceed the
# metaServer.maxRebalanceSpaceUtilThreshold
# Default is 0.82
# metaServer.maxRebalanceSpaceUtilThreshold = 0.82

# Move chunks to server below metaServer.minRebalanceSpaceUtilThreshold.
# Default is 0.72.
# metaServer.minRebalanceSpaceUtilThreshold = 0.72

# Time interval in seconds between replication queues scans.
# The more often the scan is scheduled the more cpu can potentially use.
# Default is 5 sec.
# metaServer.replicationCheckInterval = 5

# Re-balance scan depth.
# Max number of chunks to scan in one partial scan. The more chunks are scanned
# the more cpu re-balance will use, and the "faster" it will scan the chunks.
# metaServer.maxRebalanceScan = 1024

# Single re-balance partial scan time limit.
# Default is 0.03 sec.
# metaServer.maxRebalanceRunTime = 0.03

# Minimum time between two consecutive re-balance partial scans.
# Default is 0.512 sec.
# metaServer.rebalanceRunInterval = 0.512

# Max. number of a single client connection requests in flight.
# The higher value might reduce cpu and alleviate "head of the line blocking"
# when single client connection shared between multiple concurrent file readers
# and writers, potentially at the cost of reducing "fairness" between the client
# connections. Increasing the value could also reduce number of context
# switches, and os scheduling overhead with the "client" threads enabled.
# Default is 16 if the "client" threads are enabled, and 1 otherwise.
# metaServer.clientSM.maxPendingOps = 16

# ------------------ Chunk placement parameters --------------------------------

# The metaServer.sortCandidatesByLoadAvg and
# metaServer.sortCandidatesBySpaceUtilization are mutially exclusive.
# metaServer.sortCandidatesBySpaceUtilization takes precedence over
# metaServer.sortCandidatesByLoadAvg if both set to 1

# When allocating (placing) a chunk prefer chunk servers with lower "load"
# metric over the chunk servers with the higher "load" metric.
# For the write intensive file systems turning this mode on is
# recommended.
# Default is 0. Do not take chunk server "load" metric into the account.
# metaServer.sortCandidatesByLoadAvg = 0

# When allocating (placing) a chunk prefer chunk servers with lower disk space
# utilizaiton.
# Default is 0. Do not take space utilization into the account.
# metaServer.sortCandidatesBySpaceUtilization = 0

# When allocating (placing) a chunk do not consider chunk server with the "load"
# exceeding average load multiplied by metaServer.maxGoodCandidateLoadRatio.
# Default is 4.
# metaServer.maxGoodCandidateLoadRatio = 4

# When allocating (placing) a chunk do not consider chunk server with the "load"
# exceeding average "master" chunk server load multiplied by
# metaServer.maxGoodMasterLoadRatio if the chunk server is used as master (head
# or synchronous replication chain).
# Default is 4.
# metaServer.maxGoodMasterLoadRatio = 4

# When allocating (placing) a chunk do not consider chunk server with the "load"
# exceeding average "slave" load multiplied by metaServer.maxGoodSlaveLoadRatio
# if the chunk server is used as slave.
# Default is 4.
# metaServer.maxGoodSlaveLoadRatio = 4

# When allocating (placing) a chunk do not consider chunk server with the
# average number of chunks opened for write per drive (disk) exceeding average
# number of chunks opened for write across all disks / chunks servers multiplied
# by metaServer.maxWritesPerDriveRatio.
# Default is 1.5.
# metaServer.maxWritesPerDriveRatio = 1.5

# When allocating (placing) a chunk do not consider chunk server running on the
# same host as writer if the average number of chunks opened for write per drive
# (disk) exceeding average number of chunks opened for write across all disks /
# chunks servers multiplied by metaServer.maxLocalPlacementWeight.
# Default is 1.0.
# metaServer.maxLocalPlacementWeight = 1.0

# "In rack" placement for append and non append chunk allocations.
# Place chunk replicas on the same rack to save cross rack bandwidth at the cost
# of reduced reliability. Useful for temporary / scratch file systems.
# Default is 0.
# metaServer.inRackPlacementForAppend = 0

# "In rack" placement for non append files.
# Default is 0 - place replicas and chunks from the same RS blocks on different
# racks.
# metaServer.inRackPlacement = 0

#-------------------------------------------------------------------------------

# Order chunk replicas locations by the chunk "load average" metric in "get
# alloc" responses. The read client logic attempts to use replicas in this
# order.
# Default is 0. The replicas locations are shuffled randomly.
# metaServer.getAllocOrderServersByLoad = 0

# Delay recovery for the chunks that are past the logical end of file in files
# with Reed-Solomon redundant encoding.
# The delay is required to avoid starting recovery while the file is being
# written into, and the chunk sizes aren't known / final. The writer can stop
# writing into a file, and the corresponding chunks write leases might timed
# out, and will be automatically revoked. The existing writer logic sets logical
# EOF when it closes the file, before that the logical file size remains 0
# during write. (Unless it is re-write which is currently for all practical
# purposes not supported with RS files). The timeout below should be set to
# at least the max. practical file "write" time.
# Setting the timeout to a very large value will prevent processing the chunks
# sitting in the replication delayed queue from the "abandoned" files, i.e.
# files that the writer wrote something and then exited without closing the
# file.
# The parameter and the corresponding "delay" logic will likely be removed in
# future releases, and replaced with the write lease renew logic.
# Default is 6 hours or 21600 seconds.
# metaServer.pastEofRecoveryDelay = 21600

# Periodic checkpointing.
# If set to -1 checkpoint is disabled. In such case "logcompactor" can be used
# periodically create new checkpoint from the transaction logs.
# Default is 3600 sec.
# metaServer.checkpoint.interval = 3600

# Checkpoint lock file name. Can be used to serialize checkpoint write and load
# with external programs, for example logcompactor.
# Default is empty -- no lock file used.
# metaServer.checkpoint.lockFileName =

# Max consecutive checkpoint write failures.
# Meta server will exit if checkpoint write fails
# metaServer.checkpoint.maxFailedCount times in the row for any reason (not
# enough disk space for example).
# Default is 2.
# metaServer.checkpoint.maxFailedCount = 2

# Checkpoint write timeout. Max time the checkpoint write can take before
# declaring write failure.
# Default is 3600 sec.
# metaServer.checkpoint.writeTimeoutSec = 3600

# Use synchronous mode to write checkpoint, i.e. tell host os to flush all data
# to disk prior to write system call return.
# The main purpose is to reduce the number of "dirty" / unwritten pages in the
# host os vm subsystem / file system buffer cache, therefore reducing memory
# contention and lowering the chances of paging out meta server and other
# processes with no memory locking.
# Default is on.
# metaServer.checkpoint.writeSync = 1

# Checkpoint write buffer size.
# The buffer size should be adequate with synchrounous write mode enabled,
# especially if journal and data of host's file system are on the same spinning
# media device, in order to minimize the number of seeks.
# Default is 16MB.
# metaServer.checkpoint.writeBufferSize = 16777216

# ---------------------------------- Audit log. --------------------------------

# All request headers and response status are logged.
# The audit log records are null ('\0') separated.
# The log could be useful for debugging and audit purposes.
# The logging require some cpu, but the main resource consumption is disk io.
# Default is off.
# metaServer.clientSM.auditLogging = 0

# Colon (:) separated file name prefixes to store log segments.
# Default is empty list.
# metaServer.auditLogWriter.logFilePrefixes =

# Maximum log segment size.
# Default is -1 -- unlimited.
# metaServer.auditLogWriter.maxLogFileSize = -1

# Maximum number of log segments.
# Default is -1 -- unlimited.
# metaServer.auditLogWriter.maxLogFiles = -1

# Max. time to wait for the log buffer to become available.
# When wait is enabled the request processing thread will wait for the log
# buffer disk io to complete. If the disk subsystem cannot keep up with the
# logging it will slow down the meta server request processing.
# Default is -1. Do not wait, drop log record instead.
# metaServer.auditLogWriter.waitMicroSec = -1

#-------------------------------------------------------------------------------

# ---------------------------------- Message log. ------------------------------

# Message log level FATAL, ALERT, CRIT, ERROR, WARN, NOTICE, INFO, DEBUG
# Defaul is DEBUG, except for non debug builds with NDEBUG defined INFO is
# default.
metaServer.msgLogWriter.logLevel = INFO

# Colon (:) separated file name prefixes to store log segments.
# Default is empty list. The default is to use file name from the command line
# or if none specified write into file descriptor 2 -- stderror.
# metaServer.msgLogWriter.logFilePrefixes =

# Maximum log segment size.
# Default is -1 -- unlimited.
# metaServer.msgLogWriter.maxLogFileSize = -1

# Maximum number of log segments.
# Default is -1 -- unlimited.
# metaServer.msgLogWriter.maxLogFiles = -1

# Max. time to wait for the log buffer to become available.
# When wait is enabled the request processing thread will wait for the log
# buffer disk io to complete. If the disk subsystem cannot keep up with the
# logging it will slow down the meta server request processing.
# Default is -1. Do not wait, drop log record instead.
# metaServer.msgLogWriter.waitMicroSec = -1

#-------------------------------------------------------------------------------

# ===================== Chunk servers configuration parameters. ================
# Configuration parameters in the meta server configuration file take precedence
# over the chunk server configuration files.

# ---------------------------------- Message log. ------------------------------

# Chunk server log level.
chunkServer.msgLogWriter.logLevel = NOTICE

# Colon (:) separated file name prefixes to store log segments.
# Default is empty list. The default is to use file name from the command line
# or if none specified write into file descriptor 2 -- stderror.
# chunkServer.msgLogWriter.logFilePrefixes =

# Maximum log segment size.
# Default is -1 -- unlimited.
# chunkServer.msgLogWriter.maxLogFileSize = -1

# Maximum number of log segments.
# Default is -1 -- unlimited.
# chunkServer.msgLogWriter.maxLogFiles = -1

# Max. time to wait for the log buffer to become available.
# When wait is enabled the request processing thread will wait for the log
# buffer disk io to complete. If the disk subsystem cannot keep up with the
# logging it will slow down the request processing.
# For chunk servers keeping the default is strongly recommended to minimize
# dependency on the host's disk subsystem reliability and performance.
# Default is -1. Do not wait, drop log record instead.
# chunkServer.msgLogWriter.waitMicroSec = -1

#-------------------------------------------------------------------------------

# Disk io request timeout.
# Default is 270 sec. Production value is 40 sec.
# chunkServer.diskIo.maxIoTimeSec = 270

# Synchronous replication timeouts.
# Record append synchrounous replicaiton timeout.
# Default is 180 sec. Production value is 20 sec.
# chunkServer.recAppender.replicationTimeoutSec = 180
# Write replication timeout.
# Default is 300 sec. Production value is 20 sec.
# chunkServer.remoteSync.responseTimeoutSec = 300

# Controls buffered io -- use os file system cache, instead of direct io on the
# os / file systems that support direct io (most file systems on linux).
# Default is off.
# It is conceivable that enabling buffered io might help with short reads for
# the "broadcast" / "web server" type of loads. For the "typical" large io (1MB)
# requests sequential type loads enabling caching will likely lower cluster
# performance due to higher system (os) cpu overhead, and memory contention.
# Default is off.
# chunkServer.bufferedIo = 0

# If sparse files, and in particular chunks aren't used (sequential write only
# for example) the following parameter can be set to 0.
# Default is 1 -- enabled.
# chunkServer.allowSparseChunks = 1

# The minimal amount of space in bytes that must be available in order for the
# chunk directory to be used for chunk placement (considered as "writable").
# Default is chunk size -- 64MB plus chunk header size 16KB.
# chunkServer.minFsAvailableSpace = 67125248

# The minimal amount of space that must be available in order for the chunk
# directory to be used for chunk placement (considered as "writable"), expressed
# as part total host file system space.
# Default is 0.05 or 5%, or in other words stop using chunk directory when the
# host file system where the chunk directory resides reaches 95% space
# utilization.
# chunkServer.maxSpaceUtilizationThreshold = 0.05

# The "weight" of pending disk io in chunk placement.
# If set to 0 or less the pending io (number of io bytes in the disk queue) has
# no no effect on the placement (choosing chunk directory where to create chunk).
# If weight set to greater than 0, then the average pending io per chunk
# directory (host file system / disk) is calculated, as
# (total_pending_read_bytes * total_pending_read_weight +
# total_pending_write_bytes * total_pending_write_weight) / chunk_directory_count
# Chunk directories with pending_read + pending_write that exceed the value the
# above are taken out of the consideration for placement.
# Default is 0. Typical production value is 1.3
# chunkServer.chunkPlacementPendingReadWeight  = 0
# chunkServer.chunkPlacementPendingWriteWeight = 0

# Averaging interval for calculating the average time the incoming "client's"
# requests spend in io buffer wait queue. The "average wait time" value used by
# the meta server for chunk placement. The average exponentially decays (IIR
# filter).
# Default is 20 sec. Typical production value is 8.
# chunkServer.bufferManager.waitingAvgInterval  = 20

# "Not available" directories rescan interval in seconds. Default is 180 sec.
# (see comment in chunk server configuration file).
# chunkServer.dirRecheckInterval = 180
